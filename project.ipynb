{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md413FzAvFD8"
   },
   "source": [
    "# DX 704 Week 8 Project\n",
    "\n",
    "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
    "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvEjsVg10YFf"
   },
   "source": [
    "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT7nKctadu6R"
   },
   "source": [
    "## Example Code\n",
    "\n",
    "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
    "\n",
    "* https://github.com/bu-cds-omds/dx601-examples\n",
    "* https://github.com/bu-cds-omds/dx602-examples\n",
    "* https://github.com/bu-cds-omds/dx603-examples\n",
    "* https://github.com/bu-cds-omds/dx704-examples\n",
    "\n",
    "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUD8aVv44IVP"
   },
   "source": [
    "## Rover Simulator\n",
    "\n",
    "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sv0BRzHz187D"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "\n",
    "import random\n",
    "\n",
    "class RoverSimulator(object):\n",
    "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
    "\n",
    "    def __init__(self, resolution):\n",
    "        self.resolution = resolution\n",
    "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
    "\n",
    "        self.initial_states = []\n",
    "        for initial_x in (0, resolution // 2, resolution - 1):\n",
    "            for initial_y in (0, resolution // 2, resolution - 1):\n",
    "                for initial_direction in range(8):\n",
    "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
    "                    if initial_state != self.terminal_state:\n",
    "                        self.initial_states.append(initial_state)\n",
    "\n",
    "    def construct_state(self, x, y, direction):\n",
    "        assert 0 <= x < self.resolution\n",
    "        assert 0 <= y < self.resolution\n",
    "        assert 0 <= direction < 8\n",
    "\n",
    "        state = (y * self.resolution + x) * 8 + direction\n",
    "        assert self.decode_state(state) == (x, y, direction)\n",
    "        return state\n",
    "\n",
    "    def decode_state(self, state):\n",
    "        direction = state % 8\n",
    "        x = (state // 8) % self.resolution\n",
    "        y = state // (8 * self.resolution)\n",
    "\n",
    "        return (x, y, direction)\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return [-1, 0, 1]\n",
    "\n",
    "    def get_next_reward_state(self, curr_state, curr_action):\n",
    "        if curr_state == self.terminal_state:\n",
    "            # no rewards or changes from terminal state\n",
    "            return (0, curr_state)\n",
    "\n",
    "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
    "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
    "\n",
    "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
    "\n",
    "        assert curr_action in (-1, 0, 1)\n",
    "\n",
    "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
    "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
    "        next_direction = (curr_direction + curr_action) % 8\n",
    "\n",
    "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
    "        next_reward = 1 if next_state == self.terminal_state else 0\n",
    "\n",
    "        return (next_reward, next_state)\n",
    "\n",
    "    def rollout_policy(self, policy_func, max_steps=1000):\n",
    "        curr_state = self.sample_initial_state()\n",
    "        for i in range(max_steps):\n",
    "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
    "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
    "            yield (curr_state, curr_action, next_reward, next_state)\n",
    "            curr_state = next_state\n",
    "\n",
    "    def sample_initial_state(self):\n",
    "        return random.choice(self.initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LMQrlfX4Ybs",
    "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL SAMPLE 1\n"
     ]
    }
   ],
   "source": [
    "simulator = RoverSimulator(16)\n",
    "initial_sample = simulator.sample_initial_state()\n",
    "print(\"INITIAL SAMPLE\", initial_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8oSLkMqvMFF"
   },
   "source": [
    "## Part 1: Implement a Random Policy\n",
    "\n",
    "Random policies are often used to test simulators and start initial exploration.\n",
    "Implement a random policy for these simulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DewHlicn4PtW"
   },
   "outputs": [],
   "source": [
    "def random_policy(state, actions):\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJYOB9zl6szl"
   },
   "source": [
    "Use the code below to test your random policy.\n",
    "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgnNCJH453qE",
    "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR STATE 1147 ACTION 0 NEXT REWARD 0 NEXT STATE 1019\n",
      "CURR STATE 1019 ACTION -1 NEXT REWARD 0 NEXT STATE 890\n",
      "CURR STATE 890 ACTION 0 NEXT REWARD 0 NEXT STATE 890\n",
      "CURR STATE 890 ACTION 0 NEXT REWARD 0 NEXT STATE 890\n",
      "CURR STATE 890 ACTION -1 NEXT REWARD 0 NEXT STATE 889\n",
      "CURR STATE 889 ACTION -1 NEXT REWARD 0 NEXT STATE 1016\n",
      "CURR STATE 1016 ACTION -1 NEXT REWARD 0 NEXT STATE 1151\n",
      "CURR STATE 1151 ACTION 1 NEXT REWARD 0 NEXT STATE 1264\n",
      "CURR STATE 1264 ACTION 1 NEXT REWARD 0 NEXT STATE 1393\n",
      "CURR STATE 1393 ACTION -1 NEXT REWARD 0 NEXT STATE 1528\n",
      "CURR STATE 1528 ACTION -1 NEXT REWARD 0 NEXT STATE 1663\n",
      "CURR STATE 1663 ACTION -1 NEXT REWARD 0 NEXT STATE 1782\n",
      "CURR STATE 1782 ACTION -1 NEXT REWARD 0 NEXT STATE 1773\n",
      "CURR STATE 1773 ACTION 1 NEXT REWARD 0 NEXT STATE 1638\n",
      "CURR STATE 1638 ACTION 1 NEXT REWARD 0 NEXT STATE 1631\n",
      "CURR STATE 1631 ACTION -1 NEXT REWARD 0 NEXT STATE 1750\n",
      "CURR STATE 1750 ACTION -1 NEXT REWARD 0 NEXT STATE 1741\n",
      "CURR STATE 1741 ACTION -1 NEXT REWARD 0 NEXT STATE 1604\n",
      "CURR STATE 1604 ACTION 0 NEXT REWARD 0 NEXT STATE 1476\n",
      "CURR STATE 1476 ACTION 1 NEXT REWARD 0 NEXT STATE 1349\n",
      "CURR STATE 1349 ACTION 0 NEXT REWARD 0 NEXT STATE 1213\n",
      "CURR STATE 1213 ACTION 1 NEXT REWARD 0 NEXT STATE 1078\n",
      "CURR STATE 1078 ACTION 0 NEXT REWARD 0 NEXT STATE 1070\n",
      "CURR STATE 1070 ACTION 0 NEXT REWARD 0 NEXT STATE 1062\n",
      "CURR STATE 1062 ACTION -1 NEXT REWARD 0 NEXT STATE 1053\n",
      "CURR STATE 1053 ACTION 1 NEXT REWARD 0 NEXT STATE 918\n",
      "CURR STATE 918 ACTION -1 NEXT REWARD 0 NEXT STATE 909\n",
      "CURR STATE 909 ACTION 0 NEXT REWARD 0 NEXT STATE 773\n",
      "CURR STATE 773 ACTION 0 NEXT REWARD 0 NEXT STATE 645\n",
      "CURR STATE 645 ACTION 1 NEXT REWARD 0 NEXT STATE 518\n",
      "CURR STATE 518 ACTION 1 NEXT REWARD 0 NEXT STATE 519\n",
      "CURR STATE 519 ACTION 1 NEXT REWARD 0 NEXT STATE 640\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_1 = []\n",
    "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
    "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
    "    results_1.append({\n",
    "        'curr_state': curr_state,\n",
    "        'curr_action': curr_action,\n",
    "        'next_reward': next_reward,\n",
    "        'next_state': next_state\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRZOd3Bk7JIz"
   },
   "source": [
    "Submit \"log-random.tsv\" in Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to TSV file\n",
    "df_1 = pd.DataFrame(results_1)\n",
    "df_1.to_csv('submission/log-random.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAWky_dR7QK1"
   },
   "source": [
    "## Part 2: Implement Q-Learning with Random Policy\n",
    "\n",
    "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
    "Modify the rollout code to implement Q-Learning.\n",
    "Just implement one learning update for each sampled state-action in the simulation.\n",
    "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "231quBGA7pVd"
   },
   "outputs": [],
   "source": [
    "# Initialize Q-table as a dictionary\n",
    "q_2 = {}\n",
    "\n",
    "alpha = 1.0  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# Store all learning steps for the TSV file\n",
    "q_log_2 = []\n",
    "\n",
    "for episode in range(32):\n",
    "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
    "        # Get current Q-value (default to 0 if not seen before)\n",
    "        old_value = q_2.get((curr_state, curr_action), 0.0)\n",
    "        \n",
    "        # Calculate max Q-value for next state\n",
    "        next_actions = simulator.get_actions(next_state)\n",
    "        max_next_q = max([q_2.get((next_state, a), 0.0) for a in next_actions])\n",
    "\n",
    "        # Q-learning update: Q(s,a) = Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_2[(curr_state, curr_action)] = new_value\n",
    "        \n",
    "        # Log this step\n",
    "        q_log_2.append({\n",
    "            'curr_state': curr_state,\n",
    "            'curr_action': curr_action,\n",
    "            'next_reward': next_reward,\n",
    "            'next_state': next_state,\n",
    "            'old_value': old_value,\n",
    "            'new_value': new_value\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDBNOFLcPPRs"
   },
   "source": [
    "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W8cFRd7uPGqy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 32768 Q-learning updates to q-random.tsv\n",
      "Learned Q-values for 5356 state-action pairs\n"
     ]
    }
   ],
   "source": [
    "# Save Q-learning log to TSV file\n",
    "df_q_2 = pd.DataFrame(q_log_2)\n",
    "df_q_2.to_csv('submission/q-random.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Saved {len(q_log_2)} Q-learning updates to q-random.tsv\")\n",
    "print(f\"Learned Q-values for {len(q_2)} state-action pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action distribution in q-random.tsv (should be roughly equal):\n",
      "curr_action\n",
      "-1    10807\n",
      " 0    11009\n",
      " 1    10952\n",
      "Name: count, dtype: int64\n",
      "Unique actions: [np.int64(-1), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# Verify random policy generates all actions\n",
    "action_counts_2 = df_q_2['curr_action'].value_counts().sort_index()\n",
    "print(\"\\nAction distribution in q-random.tsv (should be roughly equal):\")\n",
    "print(action_counts_2)\n",
    "print(f\"Unique actions: {sorted(df_q_2['curr_action'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tnu4j4Yp72k1"
   },
   "source": [
    "Submit \"q-random.tsv\" in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMBmh7UW-vJU"
   },
   "source": [
    "## Part 3: Implement Epsilon-Greedy Policy\n",
    "\n",
    "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
    "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pS7g1sETAbKd"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, actions):\n",
    "    epsilon = 0.25\n",
    "    \n",
    "    # With probability epsilon, choose a random action (exploration)\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    \n",
    "    # Otherwise, choose the action with highest Q-value (exploitation)\n",
    "    q_values_for_state = [(action, q_3.get((state, action), 0.0)) for action in actions]\n",
    "    max_q = max(q_values_for_state, key=lambda x: x[1])[1]\n",
    "    \n",
    "    # If there are ties (common when Q-values are 0), pick randomly among best actions\n",
    "    best_actions = [action for action, q_val in q_values_for_state if q_val == max_q]\n",
    "    best_action = random.choice(best_actions)\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpSMW7CNAtEw"
   },
   "source": [
    "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5nkGhMOVJFp"
   },
   "source": [
    "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JcNQg6qRAsqc"
   },
   "outputs": [],
   "source": [
    "# Reset Q-learning state\n",
    "q_3 = {}\n",
    "\n",
    "alpha = 1.0  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# Store all learning steps for the TSV file\n",
    "q_log_3 = []\n",
    "\n",
    "# Run exactly 32 episodes and 1024 steps\n",
    "for episode in range(32):\n",
    "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
    "        # Get current Q-value (default to 0 if not seen before)\n",
    "        old_value = q_3.get((curr_state, curr_action), 0.0)\n",
    "        \n",
    "        # Calculate max Q-value for next state\n",
    "        next_actions = simulator.get_actions(next_state)\n",
    "        max_next_q = max([q_3.get((next_state, a), 0.0) for a in next_actions])\n",
    "        \n",
    "        # Q-learning update: Q(s,a) = Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_3[(curr_state, curr_action)] = new_value\n",
    "        \n",
    "        # Log this step\n",
    "        q_log_3.append({\n",
    "            'curr_state': curr_state,\n",
    "            'curr_action': curr_action,\n",
    "            'next_reward': next_reward,\n",
    "            'next_state': next_state,\n",
    "            'old_value': old_value,\n",
    "            'new_value': new_value\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 32768 Q-learning updates to q-greedy.tsv\n",
      "Learned Q-values for 4620 state-action pairs\n"
     ]
    }
   ],
   "source": [
    "# Save Q-learning log to TSV file\n",
    "df_q_3 = pd.DataFrame(q_log_3)\n",
    "df_q_3.to_csv('submission/q-greedy.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Saved {len(q_log_3)} Q-learning updates to q-greedy.tsv\")\n",
    "print(f\"Learned Q-values for {len(q_3)} state-action pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vd246wcA0HV"
   },
   "source": [
    "Submit \"q-greedy.tsv\" in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgGc8aP8DCzW"
   },
   "source": [
    "## Part 4: Extract Policy from Q-Values\n",
    "\n",
    "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
    "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "w7VnSBcYDINb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted policy for 1935 states\n",
      "Sample of policy:\n",
      "   state  action\n",
      "0      0      -1\n",
      "1      3      -1\n",
      "2      4      -1\n",
      "3      5      -1\n",
      "4      6      -1\n",
      "5      7      -1\n",
      "6     10      -1\n",
      "7     11      -1\n",
      "8     12      -1\n",
      "9     13      -1\n"
     ]
    }
   ],
   "source": [
    "# Extract policy from Q-values learned in Part 3\n",
    "# For each state that appears in q_3, find the action with highest Q-value\n",
    "\n",
    "policy_greedy = []\n",
    "\n",
    "# Get all unique states from q_3\n",
    "states_in_q3 = set(state for (state, action) in q_3.keys())\n",
    "\n",
    "for state in states_in_q3:\n",
    "    actions = simulator.get_actions(state)\n",
    "    \n",
    "    # Find action with highest Q-value for this state\n",
    "    q_values_for_state = [(action, q_3.get((state, action), 0.0)) for action in actions]\n",
    "    best_action = max(q_values_for_state, key=lambda x: x[1])[0]\n",
    "    \n",
    "    policy_greedy.append({\n",
    "        'state': state,\n",
    "        'action': best_action\n",
    "    })\n",
    "\n",
    "# Save policy to TSV file\n",
    "df_policy_greedy = pd.DataFrame(policy_greedy)\n",
    "df_policy_greedy.to_csv('submission/policy-greedy.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"Extracted policy for {len(policy_greedy)} states\")\n",
    "print(f\"Sample of policy:\")\n",
    "print(df_policy_greedy.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLcCtb64DJl-"
   },
   "source": [
    "Submit \"policy-greedy.tsv\" in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE1-nlr6Byq2"
   },
   "source": [
    "## Part 5: Implement Large Policy\n",
    "\n",
    "Train a more optimal policy using q-learning.\n",
    "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHuR4N4BD3_r"
   },
   "source": [
    "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
    "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
    "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
    "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_DWSxVHTp62"
   },
   "source": [
    "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
    "But make sure you update the q-values first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal epsilon-greedy policy that will be used during training\n",
    "def optimal_epsilon_greedy_policy(state, actions):\n",
    "    epsilon = 0.15  # Lower epsilon for more exploitation\n",
    "    \n",
    "    # With probability epsilon, choose a random action (exploration)\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    \n",
    "    # Otherwise, choose the action with highest Q-value (exploitation)\n",
    "    q_values_for_state = [(action, q_5.get((state, action), 0.0)) for action in actions]\n",
    "    best_action = max(q_values_for_state, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b1A9W4gCDiRZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training optimal policy with 2000 episodes...\n",
      "Episode 0/2000, Q-table size: 0\n",
      "Episode 500/2000, Q-table size: 4250\n",
      "Episode 1000/2000, Q-table size: 4440\n",
      "Episode 1500/2000, Q-table size: 4567\n",
      "Training complete! Q-table has 4626 state-action pairs\n",
      "Total number of states in 16x16 grid with 8 directions: 2048 states\n",
      "\n",
      "Extracted optimal policy for 1853 states\n",
      "Number of initial states in simulator: 71\n",
      "\n",
      "Sample of optimal policy:\n",
      "   state  action\n",
      "0      0      -1\n",
      "1      1      -1\n",
      "2      2       0\n",
      "3      3      -1\n",
      "4      4      -1\n",
      "5      5      -1\n",
      "6      6      -1\n",
      "7      7      -1\n",
      "8      9       1\n",
      "9     10      -1\n",
      "Episode 500/2000, Q-table size: 4250\n",
      "Episode 1000/2000, Q-table size: 4440\n",
      "Episode 1500/2000, Q-table size: 4567\n",
      "Training complete! Q-table has 4626 state-action pairs\n",
      "Total number of states in 16x16 grid with 8 directions: 2048 states\n",
      "\n",
      "Extracted optimal policy for 1853 states\n",
      "Number of initial states in simulator: 71\n",
      "\n",
      "Sample of optimal policy:\n",
      "   state  action\n",
      "0      0      -1\n",
      "1      1      -1\n",
      "2      2       0\n",
      "3      3      -1\n",
      "4      4      -1\n",
      "5      5      -1\n",
      "6      6      -1\n",
      "7      7      -1\n",
      "8      9       1\n",
      "9     10      -1\n"
     ]
    }
   ],
   "source": [
    "# Train an optimal policy using q-learning\n",
    "# Run MORE episodes to ensure coverage of all initial states\n",
    "q_5 = {}\n",
    "\n",
    "alpha = 1.0  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# Run MANY more episodes to get complete coverage of all reachable states\n",
    "num_episodes = 2000  # Increased significantly to ensure coverage\n",
    "\n",
    "print(f\"Training optimal policy with {num_episodes} episodes...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    if episode % 500 == 0:\n",
    "        print(f\"Episode {episode}/{num_episodes}, Q-table size: {len(q_5)}\")\n",
    "    \n",
    "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(optimal_epsilon_greedy_policy, max_steps=1024):\n",
    "        # Get current Q-value (default to 0 if not seen before)\n",
    "        old_value = q_5.get((curr_state, curr_action), 0.0)\n",
    "        \n",
    "        # Calculate max Q-value for next state\n",
    "        next_actions = simulator.get_actions(next_state)\n",
    "        max_next_q = max([q_5.get((next_state, a), 0.0) for a in next_actions])\n",
    "        \n",
    "        # Q-learning update: Q(s,a) = Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_5[(curr_state, curr_action)] = new_value\n",
    "        \n",
    "        if next_reward > 0:\n",
    "            # Reached terminal state, cut off this episode for speed\n",
    "            break\n",
    "\n",
    "print(f\"Training complete! Q-table has {len(q_5)} state-action pairs\")\n",
    "print(f\"Total number of states in 16x16 grid with 8 directions: {16 * 16 * 8} states\")\n",
    "\n",
    "policy_optimal = []\n",
    "\n",
    "# Get all unique states from q_5\n",
    "states_in_q5 = set(state for (state, action) in q_5.keys())\n",
    "\n",
    "for state in states_in_q5:\n",
    "    actions = simulator.get_actions(state)\n",
    "    \n",
    "    # Find action with highest Q-value for this state, with random tie-breaking\n",
    "    q_values_for_state = [(action, q_5.get((state, action), 0.0)) for action in actions]\n",
    "    max_q = max(q_values_for_state, key=lambda x: x[1])[1]\n",
    "    best_actions = [action for action, q_val in q_values_for_state if q_val == max_q]\n",
    "    best_action = random.choice(best_actions)\n",
    "    \n",
    "    policy_optimal.append({\n",
    "        'state': state,\n",
    "        'action': best_action\n",
    "    })\n",
    "\n",
    "# Save optimal policy to TSV file\n",
    "df_policy_optimal = pd.DataFrame(policy_optimal)\n",
    "df_policy_optimal.to_csv('submission/policy-optimal.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"\\nExtracted optimal policy for {len(policy_optimal)} states\")\n",
    "print(f\"Number of initial states in simulator: {len(simulator.initial_states)}\")\n",
    "print(f\"\\nSample of optimal policy:\")\n",
    "print(df_policy_optimal.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BUoHvjUDkjf"
   },
   "source": [
    "Submit \"policy-optimal.tsv\" in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smsTLuFcvR-I"
   },
   "source": [
    "## Part 6: Code\n",
    "\n",
    "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
    "You do not need to provide code for data collection if you did that by manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi8lV2pbvWMs"
   },
   "source": [
    "## Part 7: Acknowledgements\n",
    "\n",
    "If you discussed this assignment with anyone, please acknowledge them here.\n",
    "If you did this assignment completely on your own, simply write none below.\n",
    "\n",
    "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
    "\n",
    "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('submission/acknowledgements.txt', 'w') as f:\n",
    "    f.write(\"None\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "toc_visible": false
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
